\documentclass[12pt,oneside]{fithesis2}

% ===== LOADING PACKAGES =====
% language settings, main documnet language last
\usepackage[english]{babel}
% enabling new fonts support (nicer)
\usepackage{lmodern}
% setting input encoding
\usepackage[utf8]{inputenc}
% setting output encoding
\usepackage[T1]{fontenc}
% fithesis2 requires csquotes
\usepackage{csquotes}
% set page margins
\usepackage[top=3.5cm, bottom=3cm, left=2.4cm, right=2.4cm]{geometry}
% package to make bullet list nicer
\usepackage{enumitem}
% math symbols and environments
\usepackage{mathtools}
% packages for complex tables
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{dcolumn}
\usepackage{array}
% enable page rotation
\usepackage{pdflscape}
% generating hyperlinks in document
\usepackage{url}
\usepackage[plainpages=false,pdfpagelabels,colorlinks=true]{hyperref}

% ===== MAIN DOCUMENT SETTINGS =====
% adjusting hyphenation penalties
\tolerance=10000
\hyphenpenalty=500
% space between paragraphs
\setlength{\parskip}{0.6em plus0.2em minus0.2em}
% set correct spacing in itemize
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
% FI THESIS settings
\thesistitle{Usage of evolvable circuit \\for statistical testing \\of randomness}
\thesissubtitle{Bachelor thesis}
\thesisstudent{Martin Ukrop}
\thesiswoman{false}
\thesisfaculty{fi}
\thesisyear{spring 2013}
\thesisadvisor{RNDr.\ Petr Švenda,\ Ph.D.}
\thesislang{en}

% new commands for results table headers
\newcommand{\m}[1]{\multirow{2}*{#1}}
\newcommand{\rotatedHeader}[2][l]{\rotatebox{90}{\begin{tabular}[#1]{@{}l}#2\end{tabular}}}
\newcommand{\resultsTable}[1]{%
\newcolumntype{C}{>{\centering\arraybackslash}X}%
\begin{tabularx}{\textwidth}{|r*{3}{||S[table-format=2.1]|S[table-format=3.0]|C}|} \hline
\multirow{1}*{\raisebox{-\height-0.3cm}{\rotatebox{90}{\# of rounds}}} & \multicolumn{9}{c|}{IV and key reinitialization} \\ \cline{2-10}
& \multicolumn{3}{c||}{once for run} & 
\multicolumn{3}{c||}{for each test set} & 
\multicolumn{3}{c|}{for each test vector} \\ \cline{2-10}
& \multicolumn{1}{c|}{\rotatedHeader{Dieharder\\(x/20)}} & \multicolumn{1}{c|}{\rotatedHeader{\textsc{Sts Nist}\\(x/162)}} &  \rotatedHeader{EACirc\\(\textsc{aam})}
& \multicolumn{1}{c|}{\rotatedHeader{Dieharder\\(x/20)}} & \multicolumn{1}{c|}{\rotatedHeader{\textsc{Sts Nist}\\(x/162)}} & \rotatedHeader{EACirc\\(\textsc{aam})}
& \multicolumn{1}{c|}{\rotatedHeader{Dieharder\\(x/20)}} & \multicolumn{1}{c|}{\rotatedHeader{\textsc{Sts Nist}\\(x/162)}} & \rotatedHeader{EACirc\\(\textsc{aam})} \\ \hline \hline
#1
\end{tabularx}}

% ===== BEGIN DOCUMENT =====
\begin{document}

\FrontMatter
\ThesisTitlePage

\begin{ThesisDeclaration}
\DeclarationText
\AdvisorName
\end{ThesisDeclaration}

\begin{ThesisThanks}
Thanks will be here.
\end{ThesisThanks}

\begin{ThesisAbstract}
Abstract will be here.
\end{ThesisAbstract}

\begin{ThesisKeyWords}
Keywords will be here.
\end{ThesisKeyWords}

\MainMatter
\tableofcontents
\chapter{Introduction}
\label{chap:intro}

\begin{itemize}
\item problem description
\item motivation -> EACirc
\item summary of experiments to follow
\item we = team of EACirc
\item work and research done by me, if not stated otherwise (but consulted within the team!)
\item licence of EACirc, licence of thesis text
\item fithesis2 used
\end{itemize}

\chapter{Statistical randomness testing}
\label{chap:stat-rand-testing}

The goal of randomness testing is to determine, whether the data provided is \textit{random}. 
The problem comes with the definition of randomness, since in truly random data, 
each fixed subsequence (e.\,g. sequence of a hundred zeroes) has the same probability of appearing.
Thus, statistical metrics have been developed to asses the matter of randomness.

All the statistical randomness tests are based on mathematical properties that hold for
\textit{most} of the random sequences with a sufficient length.
A simple example of such property states that in each binary sequence, the number of ones and zeroes should be 
approximately the same. It is crucial to be aware, that this will not hold for \textit{all} sequences (see the example above),
but the probability of randomly generating such sequence sharply decreases when increasing the length of the assessed data.

Randomness testing based on statistical properties of data has its drawbacks and benefits, main of which are discussed below.
\begin{itemize} \rightskip=2em
\item \textbf{Speed}\\
Once the tests are implemented, they do not require excessive amount of time to perform -- 
the data is usually processed once in a linear fashion.
\item \textbf{Universality}\\
Statistical tests can by applied to any binary data regardless of its origin -- they perform equally well. 
This can be viewed both as an advantage and disadvantage, since tests cannot be effortlessly adapted to specific situations.
\item \textbf{One-way design}\\
The creation of new test must be preceded by the idea and analysis of some useful statistical property. This part can be highly 
non-trivial and usually requires a team of skilled mathematicians.
\item \textbf{Results interpretation}\\
The ever-present ambiguity in statistical measurements sometimes makes the results interpretation a highly non-trivial task.
It is crucial to understand what do the results indicate and what they do not. The above-mentioned finite sequence of binary zeroes
fails most of the statistical randomness tests, but its generation is just as probable 
as any other fixed binary sequence of the same length.
Put in another words, even the true random generator may produce non-random looking sequences once in a while.
\end{itemize}

\noindent
In practise, statistical randomness testing is being widely used in fields where the quality of random data is crucial, 
such as cryptography. To ease the assessment process, several statistical randomness testing suites have been developed, 
some of which are discussed below.

\section{Statistical Test Suite by NIST}
\label{sec:sts-nist}

Perhaps the most widely used battery of statistical tests is the Statistical Testing Suite 
by National Institute of Standards and Technology (STS NIST).
The primary motivations for developing this test suite was the need of standardised tests for detecting non-randomness 
in binary (pseudo)random sequences utilized in cryptographic applications. As well as designing the tests,
NIST provides their reference implementation and guidance in their use and application. \cite{web NIST}

The battery consists of 15 different tests, some of which can be run with several parameters. 
For detailed description of the tests, see the original documentation \cite{sts nist documentation}. 
The implementation provided by NIST supports variable input data length and arbitrary number of independent data streams. 
The testing results provide the cumulative p-value of all data streams and the number of passed runs for each test 
according to the set significance level. 
Detailed setting used for the purposes of this thesis can be found in \autoref{sec:settings-statistics}.

\section{Diehard battery of tests}
\label{sec:diehard}

The second, informal, standard of statistical randomness testing is the Diehard Battery of Tests of Randomness, 
developed by George Marsaglia over several years at Florida State University. \cite{diehard website} 
Although now becoming slightly outdated, they were one of the first and most-well known 
in the pioneering years of statistical testing of randomness. 
For long, the Diehard Battery of Tests was considered a golden standard along with STS NIST.

The battery consist of 12 different tests. The original implementation, documentation and tests description are still available,
but since the code has not been revised from its creation in 1995, we chose not to use Marsaglia's original implementation.

\section{Dieharder: A Random Number Test Suite}
\label{sec:dieharder}

Dieharder, as its predecessors, aims to ease the testing of (pseudo)random generators and data for a variety purposes in research 
and cryptography. Developed by Robert G. Brown at the Duke University, it is designed to be as extensible as possible, 
allowing easy implementation of new tests and generators for testing. Most of the tests used allow for 
modifying the default parameters, enabling advanced users to fine-tune the testing process.
According to its creators, it is intended to be the ``Swiss army knife of random number test suite'', 
or if you prefer, ``the last suite you'll ever ware'' for testing random numbers. \cite{dieharder web}

After designing the testing framework, the development team gradually reimplemented and improved the original tests from 
the Diehard Battery of Tests of Randomness (see \autoref{sec:diehard}), 
the tests from STS NIST (see \autoref{sec:sts-nist}) and began to prepare and implement their own new tests.
The suite now contains 31 different tests from various sources. Tests can be run selectively.
The testing results provide the cumulative p-value for each test and a verdict of \textsc{pass}, \textsc{weak} or \textsc{fail}
according to the set significance level.
Detailed setting used for the purposes of this thesis can be found in \autoref{sec:settings-statistics}.

\section{Drawbacks of human-designed statistical tests}
\label{sec:limits-stat-testing}

Although convenient in some ways, statistical randomness testing based on human-designed tests has several important drawbacks.
As mentioned above, the test creation must be preceded by an idea of mathematical property and its thorough analysis, 
which can be extremely time- and people-consuming. Further on, the tests are limited to one particular property and
adapting them to specific situation requires beginning the process of test creation all over again.

Both of the above-mentioned problems would be resolved if tests of comparable quality could be generated automatically, without 
the help of human specialists. Such concept and its comparison with human-generated tests is presented in the following chapters.

\chapter{Evolution-based randomness testing}
\label{chap:evo-based-testing}

In this chapter we try to describe a method of automatically generating statistical randomness tests. Compared to the standard
way of their creation, our approach would have a couple of advantages: 
\begin{itemize}
\item no prior knowledge of statistical properties of random data is needed;
\item test creation does not require excessive human analytical labour;
\item tests adapted for specific situations can be easily developed;
\item atypical and/or yet unknown data properties may be used.
\end{itemize}

\noindent
The main idea is to use supervised learning techniques based on evolutionary algorithms to design and further 
optimize a successful \textit{distinguisher} -- the test determining whether its input comes from a truly random source or not. 
The distinguisher will be represented as a hardware-like circuit consisting of a number of interconnected simple functions.
The evolution will be applied with the use of genetic programming techniques.

\section{Basic principles of genetic programming}
\label{sec:basic-ga}

Genetic programming is a biologically inspired supervised learning technique. It tries to converge to optimal 
solution by making subtle changes to previous partial solutions, assessing their impact and propagating the perspective changes
until reaching the desired success rate. The existence of partial problem solutions is therefore crucial.
The main flow of evolution implemented by genetic programming is as follows:
\begin{enumerate} \rightskip=2em
\item Firstly, a random set of partial solutions is generated. The solutions may be highly unsuccessful,
but some will nonetheless be better than other. This set of solutions is called a \textit{population}.
\item Secondly, the success of all individual solutions from the population is evaluated. The assessment is done using
the so called \textit{fitness function}. The quality of this function is crucial to the whole algorithm, as it
distinguishes the better and more successful partial solutions from the worse ones.
\item A new population of solutions is created by making a \textit{sexual crossover} from the best solutions of the 
previous generation. Informally put, solutions are subject to the survival of the fittest.
\item A small random change may be applied to some individuals in the new population. This \textit{mutation} prevents
the population from getting stuck in the local optimum and increases the chances of reaching a global optimum.
\item Steps 2-4 are iterated over and over, until the desired success rate of the population is achieved or the
required number of generations have evolved.
\end{enumerate}

\noindent
The principles of evolutionary algorithms induce a couple of design limitations and disadvantages. 
The most important ones include:
\begin{itemize} \rightskip=2em
\item Only problems with a sufficient space of partial solutions are applicable, since the individuals must be assessed 
to determine the fittest.
\item A small change in the solution should induce only a small change in the individual's fitness. If the changes were
too rapid, the evolution wouldn't be able to stabilize on the better and more successful solutions.
\item The evolution phase can be computationally very expensive, since making small changes to the individuals requires
higher number of generations evolved.
\item It may be quite difficult to fine-tune the parameters (such as population size, mutation and crossover probabilities)
to achieve the best results.
\end{itemize}

\noindent
To counterweight the drawbacks, it must be noted, that evolutionary algorithms allow us to create solution not just for particular
instance of the problem, but to the whole set of similar problems -- we may be trying to evolve a universal solver, 
rather than for the solution itself. 
This improves the computation complexity, because after an expensive learning phase, the evolved solver may be used
repeatedly on multiple instances of the problem. However, the evolution of the general solver can be trickier than it seems,
since over-learning (i.\,e. finding the solution just to the particular instance of the problem) has to be avoided.

\section{Using software-emulated circuits}
\label{sec:sw-circuits}

Our goal is to create a simple circuit performing the desired task -- distinguishing the random and non-random data streams.
Thus, let's consider solutions in the form of of a hardware-like circuit with function nodes (``gates'') 
and a set node connectors
(``wires''). Each node is responsible for computation of a simple function on its inputs (e.\,g. binary \textsc{and} operation).
Circuit nodes are positioned into layers, where outputs from one layer are connected to inputs of the next. Input of the whole
circuit is used as an input for the first layer and output of the last layer is considered the output of the entire circuit.
Connectors may only connect adjacent layers, but may cross each other (contrary to real single-layer hardware circuits).

In the current problem solution design, we consider only simple functions operating on bytes. The supported functions are:
\begin{itemize}
\item simple bit-manipulating functions (\textsc{or, and, xor, nor, nand, rotl, rotr, bitselector}),
\item simple arithmetical functions (\textsc{sum, subs, add, mult, div}),
\item identity function (\textsc{nop}) and
\item function reading specific input byte (\textsc{readx}).
\end{itemize}

\noindent
Although it would be sufficient to restrict ourselves to a smaller set of functions (e.\,g. \textsc{nand} only),
we chose to support a wider variety of functions as an human understandability trade-off.
More complex and sophisticated functions enable us to limit the circuit to significantly smaller number of layers and nodes,
while retaining a comparable expressive power.

To some extent, the structure of a software circuit resembles artificial neural networks 
(deep belief belief neural networks in particular). Notable differences are in
the learning mechanism and circuit dimensions (neural networks usually use very small number of layers). 
The function of individual nodes is different as well, since all nodes node in artificial neural networks use weighed sum.

\section{EACirc: framework for automatic problem solving}
\label{sec:eacirc-principles}

Combining the principles of genetic programming and software circuits, we developed EACirc, the framework for automatic
problem solving. The initial version of EACirc was created by (titul?) Petr Švenda at 
the Laboratory of Security and Applied Cryptography, Masaryk University \cite{labak} based on SensorSim \cite{sensorsim} application.
This initial version provided the main shared functionality: evolutionary capabilities, software circuit emulation
and basic fitness evaluation. Later on, the application was improved by Matej Pristak and Ondrej Dubovec 
(as their master and bachelor theses, respectively).

Afterwards, the object model of the entire project was redesigned and a handful of new features was added by myself. 
Most of the code that was taken over was revised and refactored as necessary
to ease the understanding of its function and to standardise naming and programming principles used throughout the project. 
Currently, the framework consist of the following main parts:
\begin{itemize}
\item \textbf{Evolutionary core}\\
The core evolutionary features are provided by GAlib, a C++ Library of Genetic Algorithm Components developed at MIT \cite{galib}.
The library, when parametrized by function callbacks (e.\,g. function for mutation, sexual crossover, fitness function, \dots),
handles the main evolutionary actions.
\item \textbf{Circuit emulator}\\
The emulator simulates the behaviour of the circuit loaded from numerical representation. It a crucial role in
fitness assessment of the population.
\item \textbf{Project modules}\\
These modules are responsible for generating the data used in circuit fitness assessment. Each module (project) corresponds to
one experiment (e.\,g. eStream candidate ciphers testing, SHA-3 candidate functions testing, \dots). The module's main
responsibility is to prepare the required number of (problem, solution) pairs in the form of circuit input stream (problem)
and optimal circuit output (solution). These pair are called a \textit{set of test vectors}.
\item \textbf{Evaluator modules}\\
Evaluator is a function responsible for yielding a numerical value of fitness, when provided with the pairs of
actual and expected circuit outputs. There are multiple approaches to evaluators -- the equality of expected and
actual output can be based on Hamming weight, numerical value, \dots
\item \textbf{Random generators}\\
Since evolutionary algorithms are highly randomized, a source of randomness is needed. To ensure the
computation determinism (all experiments need to be exactly reproducible), a hierarchy of random generators was developed.
To satisfy the varying needs, several generator types are implemented: true quantum random generator (based on pre-generated data),
configurable biased generator and low-entropy MD5-based generator.
\item \textbf{Self-tests}\\
For the ease of development, EACirc provides a handful of self-tests. Running these tests ensures the consistency
of seeding and data manipulation. Tests are implemented using CATCH, a C++ Automated Test Cases in Headers \cite{catch}.
\item \textbf{XML manipulating library}\\
Most of the files produced and processed by the framework are XML-structured files. All these files are handles via
TinyXML, a simple, small, minimal, C++ XML parser \cite{tinyxml}.
\item \textbf{Static checker}\\
Although the static checker shares some code with the main framework, it is built as an independent application.
It is designed to verify obtained results (evolved circuits) by circumventing both the genetic manipulations and circuit emulator.
\item \textbf{Miscellaneous utilities}\\
EACirc framework comes with an assortment of scripts, used mainly for downloading, checking and processing the results.
\end{itemize}

\section{Current capabilities of EACirc}
\label{sec:eacirc-capabilities}

\begin{itemize}
\item old core, basic capabilities of EA provided by GAlib and software emulation taken from work by Matej Pristak, originally from SensorSim
\item main object model has been revised to uitlize the principle of modules, thus enabling integration of multiple projects and evaluators according to actual needs.
\item guaranteed bit-reproducibility now - use of random generators revised, hierarchical system of generators created, any run can be reproduced by providing original input files and one central seed => crucial to experiment verification
\item bit-reproducibility enabled us to implement computation recommencing - EACirc can save and load its complete internal state => useful for computation-expensive experiments (when the machine is rebooted, we can continue from last saved state instead of starting allover again)
\item evolved circuits are exported to 4 different formats (from SensorSim or original work on EACirc?), binary (can be reloaded into EACirc if needed), text and graphical (using Graphviz) (used to ease humal analysis of evolved results) and C source code implementing the circuit (useful for independent verification of circuit work)
\item new static checker created, circumvents most of the EACirc framework (especially circuit emulator), used for static checking on pre-generated test vectors
\item project for distinguishing between eStream cipher output and random stream of data, taken from work by Matej Pristak and slightly revised to operate withing the new object model and allow more detailed configuration
\item project for distinguishing between SHA-3 function output and random stream of data, ideas and hash functions implementations taken from Ondrej Dubovec, gest vector geenration reimplemented from scratch
\item small project for distinguishing among external binary files
\end{itemize}

Note, that EACirc is a project beyond the scope of this thesis. Some parts were added and/or redesigned in pre process, so
different experiments may have incompatible configuration files and may have produced incomparable results.
For further details, user and development documentation, see EACirc wiki at GitHub \cite{eacirc-github}.

\chapter{Experiment settings and output data}
\label{chap:settings}

\begin{itemize}
\item introduction - this chapter will describe the settings used in the experiments...
\item our reasons for using these settings
\item description of output files
\item interpretation of result numbers
\end{itemize}

\section{EACirc settings}
\label{sec:settings-eacirc}

\begin{itemize}
\item most of the settings taken from Matej Pristak's thesis (with no optimality verification)
\item GA settings (population size 20, prob mutation 0.05, prob crosover 0.20)
\item 30000 generation with test vector sets changed every 100th generation (thus, 300 unique test sets altogether)
\item circuit - 5 layers, 8 functions in layer, input 16 bytes, output 2 bytes, maximum of 4 connectors (?), all functions allowed except for READX
\item distinguisher - correct output for stream 1 id 0x00, correct for stream 2 is 0xff
\item evaluator - "agent based", each byte is separate output, less or more than 128
\item fitness function = \#(correctly predicted vectors in set)/\#(all vectors in set)
\item 1000 test vectors in each test set
\item only distiguisher experiments, always used 500:500 (according to Matej Pristak, imballance causes test vectors to only guess what type is more frequent in current run)
\item experiment-specific settings described in appropriate section with results
\end{itemize}

\section{EACirc output data}
\label{sec:settings-eacirc-output}

\begin{itemize}
\item main goal: finding strong distinguisher (over 99\% for 50 consecutive generations)
\item displayed average stable generation across 30 independent runs \\
(stable = fitness over $99\%$ for at least next 50 test sets)
\item if none stable generation was found, average average maximum fitness after test vector change is displayed in parentheses.
\end{itemize}

\section{Random data sources}
\label{sec:settings-random}

\begin{itemize}
\item EACirc is randomized algorithm -> need good random data source
\item in most experiments a distinguisher from random data stream is being evolved => crucial to have extremely realiable random data source
\item using quantum random data, generated by measuring quantum effects
\item 2 different sources, HU Berlin, institute in Croatia
\item these 2 sources thoroughly compared, see \autoref{sec:control-germany-croatia}
\end{itemize}

\section{Settings and output data for statistical test batteries}
\label{sec:settings-statistics}

\begin{itemize}
\item to compare our results with existing statistical batteries, statistical randomness of all generated streams was checked using STS NIST and Dieharder
\item 250 MB of data generated for statistical testing, same streams used for both STS NIST and Dieharder
\item STS NIST
\begin{itemize}
\item 100 runs, 100000 bits per run (=> ~11.92 MB used for testing)
\item all tests were run, significance level of ???
\item some runs had problems with tests RandomExcursions and RandomExcursionsVariant, to ensure statistical accuracy of results, these test are ommited in results
\item for each test, following stats are computed:
\begin{itemize}
\item p-value for each run - if this p-value is out of bounds of the interval determined by the significance level, the run is considered failed
\item the number of passed runs is infered
\item the cumullative p-value of all 100 runs
\item if either the cumulative p-value or the number of passed runs for this particular test is out bounds of the interval determined by the significance level, this test is considered as failed
\end{itemize}
\item results expressed as cumullative score for the entire stream, 0 for each failed test, 1 for each passed test. expressed as part of total 162
\end{itemize}
\item Dieharder
\begin{itemize}
\item test corresponding to original Diehard (except for Diehard sums test, due to probable error in test implementation)
\item 1 run of each test, stream length determined by the test itself (comes from design of Dieharder, we want to prevent from revinding the file)
\item total data used for testing: ??? MB (smallest test: ??, biggest test: ??)
\item each test interpreted as PASSED, WEAK or FAILED (significance levels of ???)
\item results for stream again in the form of cumullative score (0 for failed, 0.5 for weak, 1 for passed), out of total 20
\end{itemize}
\end{itemize}

\chapter{Control distinguishers}
\label{chap:distinguish-control}

\begin{itemize}
\item introduction -- the need of reference numbers before analysis
\item we need to define what does it mean "indistinguishable" in our setting
\item we use quantum random data from Humboldt Universitat and Quantum random bit generator service as a standard for randomness
\end{itemize}

\section{Looking for non-randomness in quantum random data}
\label{sec:control-random-random}

\begin{itemize}
\item trying to distinguish quantum random data from quantum random data \\ => we presume to fail
\item using random data from Quantum random bit generator service
\item statistical batteries: data are random (Dieharder: 20/20, STS NIST: 188/188)
\item evolution: no stable distinguisher found, AAM of 0.52 (differences in various runs in 3rd or 4th decimal place)
\item presumption: dependence on test set size and population size
\item presumption confirmed (\autoref{tab:random-set-size-change}), AAM decreases with smaller population and bigger test set size
\end{itemize}

\begin{table}[htb]
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{|c|r||*{6}{C|}} \cline{3-8}
\multicolumn{2}{c||}{} & \multicolumn{6}{c|}{number of test vector in a set} \\ \cline{3-8}
\multicolumn{2}{c||}{} & 200 & 500 & 1000 & 2000 & 5000 & 10\,000 \\ \hline \hline
\multirow{5}*{\rotatedHeader{individuals \\ in population}}
& 5 & -- & -- & (0.509) & - & - & - \\ \cline{2-8}
& 10 & -- & -- & (0.514) & - & - & - \\ \cline{2-8}
& 20 & (0.544) & (0.527) & (0.520) & (0.514) & (0.509) & (0.506) \\ \cline{2-8}
& 50 & - & - & (0.526) & - & - & - \\ \cline{2-8}
& 100 & - & - & (0.530) & - & - & - \\ \hline
\end{tabularx}
\renewcommand{\arraystretch}{1.0}
\caption{Dependence of AAM on population size and test vector set size.}
\label{tab:random-set-size-change}
\end{table}

\section{Distinguishing quantum random data from different sources}
\label{sec:control-germany-croatia}

\begin{itemize}
\item distinguishing streams of quantum random data from Humboldt University and streams of quantum random data from Ruđer Bošković Institute
\begin{itemize}
\item Quantum Random Bit Generator Service, Centre for Informatics and Computing, Ruđer Bošković Institute, Zagreb, Croatia
\item Quantum Random Number Generator Service, Department of Physics, Humboldt University, Berlin, Germany
\end{itemize}
\item 6 files of 5 MB from each source
\item fixed initial reading offsets as (0,0) \\ => same data from given file in each run
\item looking for distinguisher for each pair
\item interpretation of results (\autoref{tab:control-germany-croatia}):
\begin{itemize}
\item data from both sources are equally random for our purposes
\item there is no single statistically different stream in these \\=>they can be used interchangeably 
\end{itemize}
\end{itemize}

\begin{table}[htb]
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{|c|r||*{6}{C|}} \cline{3-8}
\multicolumn{2}{c||}{} & \multicolumn{6}{c|}{QRBG service (Ruđer Bošković Institute, Croatia)} \\ \cline{3-8}
\multicolumn{2}{c||}{} & stream 1 & stream 2 & stream 3 & stream 4 & stream 5 & stream 6 \\ \hline \hline
\multirow{6}*{\rotatedHeader{QRNG service \\(HU, Germany)}}
& stream 1 & (0.521) & (0.520) & (0.520) & (0.519) & (0.519) & (0.519) \\ \cline{2-8}
& stream 2 & (0.518) & (0.519) & (0.520) & (0.520) & (0.520) & (0.519) \\ \cline{2-8}
& stream 3 & (0.519) & (0.522) & (0.519) & (0.520) & (0.519) & (0.519) \\ \cline{2-8}
& stream 4 & (0.520) & (0.520) & (0.519) & (0.518) & (0.519) & (0.519) \\ \cline{2-8}
& stream 5 & (0.519) & (0.520) & (0.519) & (0.518) & (0.520) & (0.520) \\ \cline{2-8}
& stream 6 & (0.520) & (0.519) & (0.520) & (0.520) & (0.519) & (0.519) \\ \hline
\end{tabularx}
\renewcommand{\arraystretch}{1.0}
\caption{Distinguishing binary quantum random streams from independent sources.}
\label{tab:control-germany-croatia}
\end{table}

\section{Uncompressed audio streams}
\label{sec:distinguishing-audio}

\begin{itemize}
\item 12 files
\begin{itemize}
\item 3 quantum random files
\item 3 noise files (white, pink, Brown), generated by SoX
\item 3 noise files via intermediate mp3 compression (2 channels, 16bits, 44.1 kHz => bitrate of 128kbps)
\item noise after 128kbs mp3 compression had ~480 kB
\item 3 samples of transcendental khaoblack metal music
\end{itemize}
\item each file is 30sec of uncompressed WAV audio (5.3MB) (including quantum random files, wav header generated by SoX)
\item evolving distinguisher for each pair
\item interpretation of results (\autoref{tab:uncompressed-audio}):
\begin{itemize}
\item quantum random stream are undistinguishable (we already know)
\item generated white noise is true (undistinguishable from random)
\item pink and Brown noise are different, distinguishable (good result, since pink and Brown noise are generated by filtering white noise and are biased towards lower frequencies)
\item different types of noise can be quite successfully distinguished from one another (generally over 75\%)
\item mp3 compression has small, but detectable effect on the sound (nearly undetectable by unskilled human ear, but successfully shifts the distinguisher success to cca 0.58)
\item comparing noise and mp3 compressed and decompressed noise of the same kind is difficult
\item used metal samples are nearly indistinguishable from each other on the binary level (although the differences are easily detectable by human ear)
\item used metal samples can be reliably distinguished from white noise (which is, in fact only a stream of random data) - general success over 80\%. Less so from pink and Brown noise - general success around 65\%.
\end{itemize}
\item most of the runs have slow rising tendency in fitness \\ => if more generations, the average maximum value might be slightly higher
\end{itemize}

\begin{landscape}
\begin{table}[p]
\centering
\newsavebox{\temp}
\newcolumntype{C}{>{\centering \begin{lrbox}{\temp} \arraybackslash}X<{\end{lrbox} \m{\unhbox\temp} \arraybackslash}}
\begin{tabularx}{22cm}{|c|>{\raggedright\arraybackslash}p{2.5cm}*{4}{||C|C|C}|} \cline{3-14}

\multicolumn{2}{l||}{} & \multicolumn{3}{c||}{random streams} & \multicolumn{3}{c||}{noise (true)} &
\multicolumn{3}{c||}{noise (via mp3)} & \multicolumn{3}{c|}{metal music} \\ \cline{3-14}

\multicolumn{2}{l||}{} &  
\multicolumn{1}{c|}{\rotatedHeader{random\\stream 1}} & 
\multicolumn{1}{c|}{\rotatedHeader{random\\stream 2}} & 
\multicolumn{1}{c||}{\rotatedHeader{random\\stream 3}} & 
\multicolumn{1}{c|}{\rotatedHeader{white noise}} & 
\multicolumn{1}{c|}{\rotatedHeader{pink noise}} & 
\multicolumn{1}{c||}{\rotatedHeader{Brown noise}} & 
\multicolumn{1}{c|}{\rotatedHeader{white noise\\(via mp3)}} & 
\multicolumn{1}{c|}{\rotatedHeader{pink noise\\(via mp3)}} & 
\multicolumn{1}{c||}{\rotatedHeader{brown noise\\(via mp3)}} & 
\multicolumn{1}{c|}{\rotatedHeader{metal music\\(sample 1)}} & 
\multicolumn{1}{c|}{\rotatedHeader{metal music\\(sample 2)}} & 
\multicolumn{1}{c|}{\rotatedHeader{metal music\\(sample 3)}} \\ \cline{3-14} \hline \hline

\multirow{3}{*}[-20pt]{\rotatedHeader{random}} &
random stream 1 & 
n/a & (0.52) & (0.52) & (0.52) & (0.80) & (0.84) & (0.59) & (0.93) & (0.89) & (0.84) & (0.87) & (0.83) \\ \cline{2-14}
& random stream 2 &
(0.52) & n/a & (0.52) & (0.52) & (0.83) & (0.83) & (0.57) & (0.82) & (0.84) & (0.90) & (0.85) & (0.82) \\ \cline{2-14}
& random stream 3 & 
(0.52) & (0.52) & n/a & (0.52) & (0.94) & (0.91) & (0.58) & (0.83) & (0.83) & (0.89) & (0.83) & (0.85) \\ \hline \hline
\multirow{3}{*}[-10pt]{\rotatedHeader{noise (true)}} & 
white noise (true) &
(0.52) & (0.52) & (0.52) & n/a & (0.83) & (0.81) & (0.59) & (0.87) & (0.89) & (0.86) & (0.93) & (0.81) \\ \cline{2-14}
& pink noise (true) &
(0.80) & (0.83) & (0.94) & (0.83) & n/a & (0.76) & (0.86) & (0.52) & (0.76) & (0.65) & (0.65) & (0.66) \\ \cline{2-14}
& Brown noise (true) &
(0.84) & (0.83) & (0.91) & (0.81) & (0.76) & n/a & (0.86) & (0.76) & (0.56) & (0.71) & (0.69) & (0.68) \\ \hline \hline
\multirow{3}{*}[-10pt]{\rotatedHeader{noise (mp3)}} & 
white noise (via mp3) &
(0.59) & (0.57) & (0.58) & (0.59) & (0.86) & (0.86) & n/a & (0.91) & (0.83) & (0.84) & (0.80) & (0.78) \\ \cline{2-14}
& pink noise (via mp3) &
(0.93) & (0.82) & (0.83) & (0.87) & (0.52) & (0.76) & (0.91) & n/a & (0.78) & (0.63) & (0.68) & (0.70) \\ \cline{2-14}
& Brown noise (via mp3) &
(0.89) & (0.84) & (0.83) & (0.89) & (0.76) & (0.56) & (0.83) & (0.78) & n/a & (0.71) & (0.69) & (0.67) \\ \hline \hline
\multirow{3}{*}[-5pt]{\rotatedHeader{metal music}} & 
metal music (sample 1) &
(0.84) & (0.90) & (0.89) & (0.86) & (0.65) & (0.71) & (0.84) & (0.63) & (0.71) & n/a & (0.54) & (0.56) \\ \cline{2-14}
& metal music (sample 2) &
(0.87) & (0.85) & (0.83) & (0.93) & (0.65) & (0.69) & (0.80) & (0.68) & (0.69) & (0.54) & n/a & (0.53) \\ \cline{2-14}
& metal music (sample 3) &
(0.83) & (0.82) & (0.85) & (0.81) & (0.66) & (0.68) & (0.78) & (0.70) & (0.67) & (0.56) & (0.53) & n/a \\ \cline{1-14}
\end{tabularx}
\caption{Distinguishing random streams and uncompressed audio (noise, compressed noise, metal music).}
\label{tab:uncompressed-audio}
\end{table}
\end{landscape}

\chapter{Distinguishing cipher outputs from random stream}
\label{chap:distinguish-cipher}

\begin{itemize}
\item introduction, idea, running EACirc along with statistical batteries
\item stream ciphers from eStream competition
\end{itemize}

\section{Stream ciphers used}
\label{sec:estream-ciphers}

\begin{itemize}
\item ciphers except for ?? (why??)
\item from last phase
\item those that could be limited in rounds are tested in weaker variant as well
\item differences from Metej Pristak thesis
\end{itemize}

\section{Generating binary stream from stream ciphers}
\label{sec:estream-settings}

\begin{itemize}
\item cipher modes (iv+key initialization frequency)
\item case of LEX (not weakening the cipher, only making shorter output)
\item case of TSC (producing binary stream of 0 for 1-8 rounds) => problems in 3 Dieharder tests
\end{itemize}

\section{Results interpretation}
\label{sec:estream-results}

\begin{itemize}
\item ???
\item more or less as statistical batteries
\item dieharder better in some case than STS-NIST (is newer and some tests are redesigned)
\item statistical tests has much more input data compared to EACirc
\item using evolved distinguisher is quick
\end{itemize}

\begin{table}[htb]
\centering
\resultsTable{
1 & 0.0 & 0 & $n=2681$ & 0.0 & 0 & (0.85) & 0.0 & 5 & $n=1431$ \\ \hline
2 & 0.5 & 0 & (0.54) & 1.0 & 0 & (0.54) & 15.5 & 146 & (0.52) \\ \hline
3 & 1.0 & 0 & (0.53) & 1.0 & 0 & (0.53) & 15.0 & 160 & (0.52) \\ \hline
4 & 3.5 & 79 & (0.52) & 3.0 & 78 & (0.52) & 20.0 & 160 & (0.52) \\ \hline
5 & 4.5 & 79 & (0.52) & 3.5 & 91 & (0.52) & 17.5 & 161 & (0.52) \\ \hline
6 & 19.0 & 158 & (0.52) & 19.0 & 159 & (0.52) & 18.0 & 162 & (0.52) \\ \hline
7 & 18.5 & 162 & (0.52) & 19.0 & 161 & (0.52) & 20.0 & 161 & (0.52) \\ \hline \hline
8 & 20.0 & 162 & (0.52) & 20.0 & 159 & (0.52) & 19.0 & 161 & (0.52) \\ \hline
}
\caption{Random distinguishers for Decim ciphertext.}
\label{tab:estream-decim}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1 & 20.0 & 162 & (0.52) & 20.0 & 161 & (0.52) & 18.0 & 162 & (0.52) \\ \hline \hline
4 & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) \\ \hline
}
\caption{Random distinguishers for FUBUKI ciphertext.}
\label{tab:estream-fubuki}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1 & 0.0 & 0 & $n=221$ & 0.0 & 0 & (0.67) & 18.5 & 162 & (0.52) \\ \hline
2 & 0.0 & 0 & $n=471$ & 0.5 & 0 & (0.66) & 20.0 & 162 & (0.52) \\ \hline
3 & 19.5 & 160 & (0.52) & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) \\ \hline \hline
13 & 20.0 & 162 & (0.52) & 20.0 & 161 & (0.52) & 19.5 & 162 & (0.52) \\ \hline
}
\caption{Random distinguishers for Grain ciphertext.}
\label{tab:estream-grain}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1 & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) \\ \hline \hline
10 & 20.0 & 160 & (0.52) & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) \\ \hline
}
\caption{Random distinguishers for Hermes ciphertext.}
\label{tab:estream-hermes}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1 & 0.0 & 0 & $n=148$ & 0.0 & 0 & $n=7274$ & 3.0 & 1 & $n=154$ \\ \hline
2 & 4.0 & 1 & $n=221$ & 4.0 & 1 & $n=304$ & 3.5 & 1 & $n=254$ \\ \hline
3 & 0.5 & 1 & $n=378$ & 3.5 & 1 & $n=491$ & 4.0 & 1 & $n=361$ \\ \hline
4 & 20.0 & 162 & (0.52) & 19.5 & 162 & (0.52) & 20.0 & 161 & (0.52) \\ \hline \hline
10 & 19.5 & 162 & (0.52) & 19.5 & 160 & (0.52) & 20.0 & 160 & (0.52) \\ \hline
}
\caption{Random distinguishers for LEX ciphertext.}
\label{tab:estream-lex}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1 & 5.5 & 1 & (0.87) & 8.5 & 1 & (0.67) & 17.5 & 161 & (0.52) \\ \hline
2 & 5.5 & 1 & (0.87) & 7.0 & 1 & (0.67) & 19.5 & 162 & (0.52) \\ \hline
3 & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) & 19.5 & 161 & (0.52) \\ \hline \hline
12 & 20.0 & 162 & (0.52) & 19.5 & 161 & (0.52) & 19.0 & 161 & (0.52) \\ \hline
}
\caption{Random distinguishers for Salsa20 ciphertext.}
\label{tab:estream-salsa}
\end{table}

\begin{table}[htb]
\centering
\resultsTable{
1--8 & 0.0${}^{*}$ & 0 & $n=104$ & 0.0${}^{*}$ & 0 & $n=101$ & 0.0${}^{*}$ & 0 & $n=104$ \\ \hline
9 & 1.0 & 1 & $n=234$ & 1.5 & 1 & $n=491$ & 2.0 & 1 & $n=121$ \\ \hline
10 & 2.0 & 13 & $n=188$ & 3.0 & 13 & $n=218$ & 3.0 & 12 & $n=158$ \\ \hline
11 & 10.0 & 157 & (0.52) & 11.5 & 157 & (0.52) & 14.0 & 159 & (0.52) \\ \hline
12 & 16.0 & 162 & (0.52) & 17.0 & 161 & (0.52) & 17.5 & 162 & (0.52) \\ \hline
13 & 20.0 & 162 & (0.52) & 20.0 & 162 & (0.52) & 19.0 & 162 & (0.52) \\ \hline \hline
32 & 20.0 & 161 & (0.52) & 20.0 & 162 & (0.52) & 20.0 & 161 & (0.52) \\ \hline
}
\caption{Random distinguishers for TSC-4 ciphertext.}
\label{tab:estream-tsc}
\end{table}

\chapter{Analysis of Salsa20 output stream}
\label{chap:analysis-salsa}
\begin{itemize}
\item learns current vectors quicker than other ciphers
\item the case of six
\end{itemize}

\chapter{Distinguishing hash outputs from random stream}
\label{chap:distinguish-hash}

\begin{itemize}
\item introduction, idea
\item hash function candidates from SHA-3
\end{itemize}

\section{Hash functions used}
\label{sec:hash-functions}

\begin{itemize}
\item except for 2 (?? source code size, compilation)
\item from last phase
\item those that could be limited in rounds are tested in weaker variant as well
\item differences from Ondrej Dubovec Bc thesis
\end{itemize}

\section{Generating binary stream from hash functions}
\label{sec:hash-settings}

\begin{itemize}
\item length set to 256b
\item hashing 4 byte counters starting from random value (in fact, cutting each hash in half)
\end{itemize}

\section{Determining optimal set change frequency}
\label{sec:hash-set-change-freqency}

\begin{itemize}
\item previously,we used change every 100 generations
\item 100 was taken from Matej Pristak's thesis
\item Ondrej proposes 10 as best, however, data is not provided
\item interpretation of results (\autoref{tab:hash-set-change-freqency}):
\begin{itemize}
\item ???
\end{itemize}
\end{itemize}

\begin{table}[htb]
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{2cm}||*{8}{C|}} \cline{2-9}
\multicolumn{1}{l||}{} & \multicolumn{8}{c|}{change frequency for test vector set} \\ \cline{2-9}
\multicolumn{1}{l||}{} & 5 & 10 & 20 & 50 & 100 & 200 & 500 & 1000 \\ \hline \hline
30\,000 g. & (0.614) & (0.614) & (0.607) & (0.602) & (0.599) & (0.598) & (0.591) & (0.582) \\ \hline
run-time & 70 m. & 52 m. & 42 m. & 37 m. & 32 m. & 28 m. & 23 m. & 20 m. \\ \hline \hline
300 sets & (0.567) & (0.583) & (0.585) & (0.589) & (0.599) & (0.608) & (0.617) & (0.618) \\ \hline
run-time & 4 m. & 6 m. & 9 m. & 19 m. & 32 m. & 57 m. & 115 m. & 220 m. \\ \hline
\end{tabularx}
\renewcommand{\arraystretch}{1.0}
\caption{Determining optimal change frequency for test vector set.}
\label{tab:hash-set-change-freqency}
\end{table}

\section{Results interpretation}
\label{sec:hash-results}

\begin{itemize}
\item ???
\end{itemize}

\begin{table}[htb]
\centering
%\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{2cm}||*{7}{C|}} \cline{2-8}
\multicolumn{1}{l||}{} & \multicolumn{7}{c|}{number of rounds} \\ \cline{2-8}
\multicolumn{1}{l||}{} & 0 & 1 & 2 & 3 & 4 & 5 & full \\ \hline \hline
ARIRANG & $n=694$ & $n=707$ & $n=467$ & $n=1071$ & (full) & -- & (0.52) \\ \hline
Aurora & $n=5614$ & (0.75) & (0.78) !!! & (0.52) & -- & -- & (0.52) \\ \hline
Blake & $n=474$ & (0.52) & -- & -- & -- & -- & (0.52) \\ \hline
Blue Midnight Wish & (0.52) & -- & -- & -- & -- & -- & (0.52) \\ \hline
Cheetah & $n=181$ & $n=574$ & $n=708$ & (0.90) !!! & (0.86)!!! & (0.52) & (0.52) \\ \hline
CHI & (0.52) & -- & -- & -- & -- & -- & (0.52) \\ \hline
CRUNCH & $n=104$ & $n=534$ & $n=954$ & 10-$n=1327$ & 17-$n=774$ & 34-(0.52) & (0.52) \\ \hline
CubeHash & $n=104$ & (0.52) & -- & -- & -- & -- & (0.52) \\ \hline
DCH & $n=104$ & (0.73) !!! & (0.52) & -- & -- & -- & (0.52) \\ \hline
Dynamic SHA & $n=484$ & $n=2337$ & $n=1773$ !!! & (0.95) !!! & (0.74) & (0.61) & (0.52) \\ \hline
Dynamic SHA & from 6 -> & (0.59) & & & & & \\ \hline
Dynamic SHA2 & -- & (0.94) !!! & (0.74) & (0.75) & (0.57) & (0.60) & (0.52) \\ \hline
Dynamic SHA2 & from 6 -> & & & & & & \\ \hline
\end{tabularx}
%\renewcommand{\arraystretch}{1.0}
\caption{Random distinguishers for SHA-3 candidate functions.}
\label{tab:hash-distinguishers}
\end{table}

\begin{table}[htb]
\centering
%\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}p{2cm}||*{7}{C|}} \cline{2-8}
\multicolumn{1}{l||}{} & \multicolumn{7}{c|}{number of rounds} \\ \cline{2-8}
\multicolumn{1}{l||}{} & 0 & 1 & 2 & 3 & 4 & 5 & full \\ \hline \hline
ECHO & -- & (0.73) !!! & & & & & (0.52) \\ \hline
ESSENCE & 8-(0.52) & & & & & & (0.52) \\ \hline
Fugue & (0.52) & & & & & & (0.52) \\ \hline
Grøstl & $n=8651$ !!! & & & & & & (0.52) \\ \hline
Hamsi & $n=12408$ !!! & & & & & & (0.52) \\ \hline
JH & $n=581$ & & & & & & (0.52) \\ \hline
Lesamnta & $n=791$ & & & & & & (0.52) \\ \hline
Luffa & $n=604$ & & & & & & (0.52) \\ \hline
MD6 & $n=101$ & & & & & & (0.52) \\ \hline
Sarmal & (0.52) & & & & & & (0.52) \\ \hline
SHAvite-3 & (0.52) & & & & & & (0.52) \\ \hline
SIMD & $n=5428$ & & & & & & (0.52) \\ \hline
Tangle & $n=714$ & & & & & & (0.52) \\ \hline
Twister & $n=474$ & & & & & & (0.52) \\ \hline
Vortex & $n=104$ & & & & & & $n=1257$ \\ \hline
WaMM & $n=1171$ & & & & & & (0.52) \\ \hline
Waterfall & (0.52) & & & & & & (0.52) \\ \hline
\end{tabularx}
%\renewcommand{\arraystretch}{1.0}
\caption{Random distinguishers for SHA-3 candidate functions.}
\label{tab:hash-distinguishers2}
\end{table}

\chapter{Conclusions and future work}
\label{chap:conclusions}



\section{Conclusions based on experimental data}
\label{sec:outro-conclusions}

\begin{itemize}
\item summary of what we did
\item control distinguishers (random-random, hr-de, audio)
\item estream (round limited ciphers)
\item analysis of Salsa20
\item sha3 (round limited hash functions)
\end{itemize}

\begin{itemize}
\item different approach than statistical batteries -> possibly new things
\item dynamically adapting distinguisher - both advantage and disadvantage
\item comparable to statistical tests, however smaller inputs
\item speed: slow learning (more computational power needed), fast distinguishing
\item problem with interpreting results
\end{itemize}

\section{Proposed future work}
\label{sec:outro-future-work}

\begin{itemize}
\item deep analyses instead of wide
\item possibilities of longer input 
\begin{itemize}
\item READX
\item memory circuit
\end{itemize}
\item tools for interpreting results
\begin{itemize}
\item histogram of outputs in nodes
\end{itemize}
\item fixing functions in layers
\end{itemize}

\end{document}
